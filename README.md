# Cheat Sheet de **Ollama** y uso de **DeepSeek** (Linux)

> GuÃ­a rÃ¡pida para instalar, configurar y usar modelos locales con **Ollama** en Linux, con ejemplos listos para **DAW** y **DAM**.

---

## ðŸ§© InstalaciÃ³n en Linux (Ubuntu / Linux Lite)

### OpciÃ³n 1 â€” Instalador oficial (recomendada)
```bash
curl -fsSL https://ollama.com/install.sh | bash
# Si usas systemd:
sudo systemctl enable ollama
sudo systemctl restart ollama
```

### OpciÃ³n 2 â€” Binario manual (cuando no puedes usar curl | bash)
1) Descarga el binario de tu arquitectura desde la pÃ¡gina oficial.  
2) Copia el ejecutable a `/usr/local/bin/ollama` y dale permisos `755`.  
3) Crea el servicio systemd (opcional) para iniciarlo al arrancar.

> Puerto por defecto de la API: **http://localhost:11434**

---

## ðŸ§ª VerificaciÃ³n rÃ¡pida
```bash
# Ver versiÃ³n
ollama --version

# Descargar un modelo pequeÃ±o (rÃ¡pido en CPU)
ollama pull llama3:8b

# Probar en modo interactivo
ollama run llama3:8b
```

> Con CPU como **Intel i3-10110U** empieza con modelos de **7B/8B** o variantes cuantizadas. Si notas lentitud, baja `num_ctx` (p. ej. 2048) o usa `q4` cuando estÃ© disponible.

---

## ðŸ§° Comandos esenciales
```bash
# Descargar / actualizar modelo
ollama pull deepseek-r1:1.5b

# Listar modelos locales (ya descargados)
ollama list

# Ver detalles de un modelo
ollama show llama3:8b

# Ejecutar de forma interactiva (mantiene contexto en esa sesiÃ³n)
ollama run deepseek-r1:1.5b

# Ejecutar con prompt de una sola vez (no hay contexto entre invocaciones)
echo "Explica la fotosÃ­ntesis" | ollama run llama3:8b

# Ver procesos / detener
ollama ps
ollama stop llama3:8b

# Eliminar un modelo
ollama rm llama3:8b
```

---

## ðŸŒ Â¿DÃ³nde ver mÃ¡s modelos disponibles?
ðŸ‘‰ CatÃ¡logo oficial de modelos: [https://ollama.com/library](https://ollama.com/library)  

AllÃ­ encontrarÃ¡s nombres, tamaÃ±os y variantes listos para usar.  
Algunos ejemplos:
- `llama3:8b`, `llama3:70b`
- `mistral:7b`
- `codellama:7b`
- `qwen2:7b`
- `deepseek-r1:1.5b`

Para instalarlos:
```bash
ollama pull nombre_del_modelo
```

---

## ðŸ§  Â¿QuÃ© es un `Modelfile`?
Archivo que define **cÃ³mo corre el modelo** (base + parÃ¡metros + sistema).

```dockerfile
FROM llama3:8b
SYSTEM "Eres un asistente Ãºtil y conciso que responde en espaÃ±ol."
PARAMETER temperature 0.7
PARAMETER num_ctx 4096
```

Crear el modelo y usarlo:
```bash
ollama create nuevo-contexto -f Modelfile
ollama run nuevo-contexto
```

---

## ðŸ”§ ParÃ¡metros Ãºtiles (resumen)
| ParÃ¡metro | Para quÃ© sirve | RecomendaciÃ³n |
|---|---|---|
| `temperature` | Creatividad | 0.2â€“0.4 tÃ©cnico, 0.7â€“1.0 creativo |
| `num_ctx` | Memoria de contexto | 2048â€“4096 en CPU modesta |
| `top_p` | Nucleus sampling | 0.8â€“0.95 |
| `top_k` | Tokens candidatos | 20â€“50 |
| `repeat_penalty` | Evitar repeticiones | 1.1â€“1.3 |
| `num_predict` | MÃ¡x. tokens de salida | 128â€“512 segÃºn caso |
| `stop` | Cortar salida | por ejemplo `###` |

---

## ðŸ§µ Contexto (sesiÃ³n vs. tuberÃ­as)
- **Interactivo** (`ollama run modelo`): mantiene contexto **hasta salir**.
- **Comando** (`echo | ollama run`): **no** mantiene contexto. Para simularlo, **reenvÃ­a el historial**.

```bash
# Historial acumulado
printf "Usuario: Explica la fotosÃ­ntesis.\n" > conversacion.txt
cat conversacion.txt | ollama run nuevo-contexto

# AÃ±adir pregunta y reenviar
printf "Usuario: Â¿RelaciÃ³n con la respiraciÃ³n celular?\n" >> conversacion.txt
cat conversacion.txt | ollama run nuevo-contexto
```

---

## ðŸ§ª API HTTP (localhost:11434)
```bash
curl http://localhost:11434/api/generate \
  -d '{
    "model": "llama3:8b",
    "prompt": "Resume en 3 puntos la fotosÃ­ntesis.",
    "options": { "temperature": 0.4, "num_ctx": 4096, "num_predict": 160 }
  }'
```

---

## ðŸ§  DeepSeek en Ollama (siempre en espaÃ±ol)
```dockerfile
FROM deepseek-r1:1.5b
PARAMETER temperature 0.3
PARAMETER num_ctx 8192
SYSTEM "Eres un asistente de IA. RESPONDE SIEMPRE EN ESPAÃ‘OL, con claridad tÃ©cnica."
```

### Presets por ciclo formativo
**DAW**
```dockerfile
FROM llama3:8b
SYSTEM "Mentor DAW. HTML5/CSS/JS(ES6+), Node/Express, REST/JSON, CORS/CSRF. Responde en espaÃ±ol con ejemplos ejecutables."
PARAMETER temperature 0.5
PARAMETER num_ctx 4096
```

**DAM**
```dockerfile
FROM llama3:8b
SYSTEM "Mentor DAM. Java/Kotlin, Android, SQLite/Room, MVVM, pruebas, arquitectura limpia. Incluye snippets y comandos Gradle."
PARAMETER temperature 0.5
PARAMETER num_ctx 4096
```

---

## ðŸ§© Plantillas y JSON estructurado
```dockerfile
FROM llama3:8b
SYSTEM "Responde solo en JSON vÃ¡lido."
RESPONSE_FORMAT json
PARAMETER temperature 0.3
```

---

## âš¡ Rendimiento y soluciÃ³n de problemas
- **CPU lenta**: usa 7B/8B, baja `num_ctx` a 2048â€“3072 y `num_predict` a 128â€“256.
- **Salida se corta**: sube `num_predict`.
- **Repite mucho**: baja `temperature`, sube `repeat_penalty`.
- **GPU (opcional)**: si tienes NVIDIA/CUDA o AMD ROCm instalados, Ollama puede usarlos automÃ¡ticamente.
- **Logs del servicio**: `journalctl -u ollama -f` (o alias `ollamalogs`).

---

## ðŸ“š Enlaces Ãºtiles
- CatÃ¡logo de modelos: https://ollama.com/library
- Modelfile (docs): https://github.com/ollama/ollama/blob/main/docs/modelfile.md
- API: https://github.com/ollama/ollama/blob/main/docs/api.md
- Modelos populares: `llama3`, `mistral`, `qwen2`, `codellama`, `deepseek-r1`
