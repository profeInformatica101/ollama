# Cheat Sheet de Ollama y uso de DeepSeek

Este documento sirve como una gu√≠a r√°pida para el uso de **Ollama** y **DeepSeek**.

---

## üöÄ Ollama

### Instalaci√≥n
# üß† Ollama Cheat Sheet + Ejemplos de Modelfile

üìò **Documentaci√≥n oficial:**  
https://github.com/ollama/ollama/blob/main/docs/modelfile.md  

---

# üöÄ ¬øQu√© es un `Modelfile`?

Es un archivo que define c√≥mo se ejecuta un modelo en **Ollama**.  
Permite configurar instrucciones, par√°metros de rendimiento y comportamiento.  

Un `Modelfile` se compone de:

- **FROM** ‚Üí Modelo base  
- **PARAMETER** ‚Üí Ajustes del modelo  
- **SYSTEM / TEMPLATE / RESPONSE_FORMAT** ‚Üí Instrucciones y comportamiento  
- **LICENSE / TEMPLATES / ADAPTERS** ‚Üí Opcional para casos avanzados  

---

# ‚öôÔ∏è Ejemplo B√°sico de `Modelfile`

```Dockerfile
FROM llama2

# Mensaje inicial del sistema (instrucciones fijas)
SYSTEM """
Eres un asistente √∫til y conciso que responde en espa√±ol.
"""

# Par√°metros comunes
PARAMETER temperature 0.7
PARAMETER num_ctx 4096

```
## üîß Par√°metros de `Modelfile` en Ollama

| Par√°metro        | Descripci√≥n                                                                 | Instrucciones / Cu√°ndo usarlo                                                                 | Ejemplo |
|------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------|
| `temperature`    | Controla la **creatividad** de las respuestas.<br>0 = determinista, >1 = m√°s creativo | Usa valores bajos (0.2‚Äì0.4) para precisi√≥n t√©cnica.<br>Valores altos (0.8‚Äì1.2) para creatividad o brainstorming. | `PARAMETER temperature 0.8` |
| `num_ctx`        | Tama√±o del **contexto** (tokens que el modelo puede recordar).              | Aumenta si trabajas con documentos largos o c√≥digo extenso.<br>Por defecto ~4096 tokens.       | `PARAMETER num_ctx 8192` |
| `top_p`          | **Nucleus sampling**: limita la selecci√≥n a los tokens m√°s probables (p).   | Recomendado entre 0.8‚Äì0.95 para equilibrio entre precisi√≥n y diversidad.                      | `PARAMETER top_p 0.9` |
| `top_k`          | Selecci√≥n de los **K tokens m√°s probables**.                               | Valores bajos ‚Üí m√°s determinista.<br>Valores altos ‚Üí m√°s creativo.                            | `PARAMETER top_k 40` |
| `repeat_penalty` | Penaliza repeticiones de palabras/tokens.                                  | √ötil si el modelo repite frases. Valores entre 1.1‚Äì1.3 son comunes.                           | `PARAMETER repeat_penalty 1.2` |
| `stop`           | Define tokens de **parada** (detiene la generaci√≥n).                       | Muy √∫til para delimitar respuestas o cortar en un prompt espec√≠fico.                          | `PARAMETER stop "###"` |
| `num_predict`    | M√°ximo de **tokens generados** por respuesta.                              | Ajusta para limitar la longitud de la salida. Ej: 200 tokens ‚âà p√°rrafo corto.                 | `PARAMETER num_predict 200` |


## üßµ Crear un hilo de conversaci√≥n con contexto

En Ollama, un **hilo de conversaci√≥n** significa que el modelo recuerda lo que se habl√≥ antes para dar continuidad a las respuestas.  
Esto se logra configurando adecuadamente el `Modelfile` y usando el par√°metro **`num_ctx`**, que define cu√°ntos tokens (palabras + s√≠mbolos) puede recordar el modelo.

---

### Ejemplo de par√°metros

Imagina que el modelo es como un estudiante tomando apuntes en clase:  
- **`num_ctx`** es el tama√±o del cuaderno ‚Üí mientras m√°s grande, m√°s puede recordar.  
- **El historial de conversaci√≥n** son los apuntes previos ‚Üí el modelo los usa para responder sin olvidar lo dicho antes.  
- Si el cuaderno es peque√±o (`num_ctx` bajo), el modelo olvidar√° lo que escribiste al inicio.  
- Si es grande (`num_ctx` alto), podr√° recordar todo el hilo de conversaci√≥n.

---

### C√≥mo usarlo en la pr√°ctica
## 1. Crear el archivo Modelfile con el contenido anterior:
```
nano Modelfile
```
## 2. Crear el modelo personalizado:
```
ollama create nuevo-contexto -f Modelfile
```
nuevo-contexto es el nombre del modelo personalizado

## 3. Ejecutarlo e iniciar la conversaci√≥n:
```
ollama run nuevo-contexto
```

## 4. Ejemplo de di√°logo en Linux con `cat` y tuber√≠as**

### Opci√≥n A ‚Äî Una sola pregunta (sin contexto entre runs)
```bash
echo "Expl√≠came qu√© es la fotos√≠ntesis." > pregunta1.txt
```

En Linux, podemos simular un di√°logo creando archivos de texto y envi√°ndolos al modelo con `cat` y el operador `|`.

---
### Opci√≥n B ‚Äî Simular contexto enviando el historial acumulado
Como ollama run es stateless entre ejecuciones, reenv√≠a el hist√≥rico completo.
1. Crea un archivo con el hist√≥rico:
```bash
cat > conversacion.txt << 'EOF'
Usuario: Explica qu√© es la fotos√≠ntesis.
EOF
```
2. Env√≠alo al modelo:
```bash
cat conversacion.txt | ollama run nuevo-contexto
```

3. A√±ade una nueva pregunta al mismo archivo y vuelve a enviarlo:
```bash
cat >> conversacion.txt << 'EOF'
Usuario: ¬øY c√≥mo se relaciona con la respiraci√≥n celular?
EOF

cat conversacion.txt | ollama run nuevo-contexto
```

### Opci√≥n C ‚Äî Dentro de una sesi√≥n (recomendado)
```bash
ollama run nuevo-contexto
# ahora pregunta, recibe respuesta, y vuelve a preguntar SIN salir
```


# Otros ejemplos de Modelfile por caso de uso
## DeepSeek en Ollama (responder siempre en espa√±ol)

```Dockerfile
FROM deepseek-r1:1.5b
PARAMETER temperature 0.3
PARAMETER num_ctx 8192
SYSTEM "Eres un asistente de IA. RESPONDE SIEMPRE EN ESPA√ëOL, con claridad t√©cnica."
```

## üß© Otro ejemplos de Modelfile por caso de uso

```Dockerfile
FROM deepseek-r1:1.5b

# Par√°metros
PARAMETER temperature 0.3
#PARAMETER num_predict 200

# Instrucciones iniciales: idioma espa√±ol obligatorio
SYSTEM """
Eres un asistente de inteligencia artificial. 
RESPONDE SIEMPRE EN ESPA√ëOL, sin utilizar nunca el ingl√©s. 
Todas las respuestas deben ser claras, t√©cnicas y adaptadas a usuarios hispanohablantes.
"""
```


##  üìö Estilo Profesor (explicaciones largas y detalladas)
```Dockerfile
FROM llama2
SYSTEM """
Eres un profesor paciente. Explica de forma clara y con ejemplos detallados.
"""
PARAMETER temperature 0.8
PARAMETER num_ctx 8192

```

## üß∞ Comandos b√°sicos de Ollama

```bash
# Descargar un modelo
ollama pull llama3:8b

# Ejecutar un modelo en modo interactivo (mantiene contexto en esa sesi√≥n)
ollama run llama3:8b

# Ejecutar con prompt directo (una sola vez)
echo "Explica la fotos√≠ntesis" | ollama run llama3:8b

# Listar modelos locales
ollama list

# Ver procesos en ejecuci√≥n
ollama ps

# Detener un proceso por nombre o ID
ollama stop llama3:8b

# Eliminar un modelo local
ollama rm llama3:8b

# Crear un modelo desde un Modelfile
ollama create mi-modelo -f Modelfile



```markdown
## üß≠ Elegir modelo y variantes

- **Nombres comunes**: `llama3:8b`, `llama3:70b`, `mistral:7b`, `codellama:7b`, `qwen2:7b`, `deepseek-r1:1.5b`.
- **Tama√±o**: 7B/8B = r√°pido y ligero; 13B‚Äì70B = mejor calidad pero m√°s memoria.
- **Cuantizaci√≥n**: variantes m√°s ‚Äúligeras‚Äù consumen menos RAM (p. ej. etiquetas `:q4`/`q5` seg√∫n el modelo).
- **Consejo**: empieza con 7B/8B; si necesitas m√°s calidad, sube gradualmente.

## üßµ Contexto: sesi√≥n vs. tuber√≠as

- **Interactivo** (`ollama run modelo`) ‚áí **s√≠** mantiene el contexto hasta que sales.
- **Comando √∫nico** (`echo "..." | ollama run modelo`) ‚áí **no** mantiene contexto entre invocaciones.
- Para ‚Äúsimular‚Äù contexto con tuber√≠as, **reenv√≠a el historial completo** cada vez:

```bash
# Crear historial
printf "Usuario: Explica la fotos√≠ntesis.\n" > conversacion.txt
cat conversacion.txt | ollama run nuevo-contexto

# A√±adir nueva pregunta
printf "Usuario: ¬øRelaci√≥n con la respiraci√≥n celular?\n" >> conversacion.txt
cat conversacion.txt | ollama run nuevo-contexto


```markdown
## üß™ API HTTP de Ollama (localhost:11434)

> √ötil para apps, scripts y automatizaci√≥n.

### Generaci√≥n (texto √∫nico)
```bash
curl http://localhost:11434/api/generate \
  -d '{
    "model": "llama3:8b",
    "prompt": "Resume en 3 puntos la fotos√≠ntesis.",
    "options": { "temperature": 0.4, "num_ctx": 4096, "num_predict": 160 }
  }'
```



## üß™ API HTTP de Ollama (localhost:11434)

> √ötil para apps, scripts y automatizaci√≥n.

### Generaci√≥n (texto √∫nico)
```bash
curl http://localhost:11434/api/generate \
  -d '{
    "model": "llama3:8b",
    "prompt": "Resume en 3 puntos la fotos√≠ntesis.",
    "options": { "temperature": 0.4, "num_ctx": 4096, "num_predict": 160 }
  }'
```


```markdown
## üß© Plantillas y salida en JSON

### Forzar JSON estructurado
```Dockerfile
FROM llama3:8b
SYSTEM "Responde solo en JSON v√°lido."
RESPONSE_FORMAT json
PARAMETER temperature 0.3
```


```markdown
## ‚ûï Par√°metros adicionales √∫tiles

| Par√°metro            | Para qu√© sirve                                | Ejemplo                          |
|---------------------|-----------------------------------------------|----------------------------------|
| `seed`              | Fijar aleatoriedad (reproducible)             | `PARAMETER seed 42`              |
| `presence_penalty`  | Evita repetir **temas**                        | `PARAMETER presence_penalty 0.8` |
| `frequency_penalty` | Evita repetir **palabras**                     | `PARAMETER frequency_penalty 0.5`|
| `mirostat`          | Control de coherencia (0=off, 1 √≥ 2 = on)     | `PARAMETER mirostat 2`           |
| `mirostat_tau`      | Temperatura objetivo de Mirostat               | `PARAMETER mirostat_tau 5.0`     |
| `mirostat_eta`      | Velocidad de ajuste Mirostat                   | `PARAMETER mirostat_eta 0.1`     |

## ‚ö° Rendimiento y soluci√≥n de problemas

- **Memoria insuficiente**: usa modelos m√°s peque√±os (7B/8B) o cuantizados, baja `num_ctx`.
- **Salida se corta**: sube `num_predict`.
- **Respuestas muy largas/repetitivas**: baja `temperature`, sube `repeat_penalty`.
- **GPU**: aseg√∫rate de tener drivers y CUDA/Metal seg√∫n tu SO (Ollama lo detecta autom√°ticamente si es posible).
- **Lento en CPU**: reduce `top_k`, `num_ctx`, o usa variantes cuantizadas.

## üß† DeepSeek (nota y ejemplos)

- **Local** con Ollama: `FROM deepseek-r1:1.5b` en tu `Modelfile`.
- **En la nube** (API oficial) ‚áí requiere **API key** y conexi√≥n a Internet.

### Python (cliente ficticio como ejemplo pedag√≥gico)
```python
from deepseek import DeepSeek
ds = DeepSeek(api_key="TU_API_KEY")
print(ds.chat("Explica aprendizaje profundo en 3 l√≠neas."))


```markdown
## üéØ Presets por ciclo formativo

### DAW ‚Äî Desarrollo de Aplicaciones Web
```Dockerfile
FROM llama3:8b
SYSTEM "Mentor DAW. HTML5/CSS/JS(ES6+), Node/Express, REST/JSON, CORS/CSRF. Responde en espa√±ol con ejemplos ejecutables."
PARAMETER temperature 0.5
PARAMETER num_ctx 8192
```

```Dockerfile
FROM llama3:8b
SYSTEM "Mentor DAM. Java/Kotlin, Android, SQLite/Room, MVVM, tests, arquitectura limpia. Incluye snippets y comandos Gradle."
PARAMETER temperature 0.5
PARAMETER num_ctx 8192
```


